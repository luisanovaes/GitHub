{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisanovaes/GitHub/blob/master/%5BRodar_a_topifica%C3%A7%C3%A3o%5D_topic_model_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htl8qIkMZ8h1",
        "outputId": "9d18c1c6-11e5-475c-9d71-cfe1de708733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bbc591726153>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Acesso ao drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "#Acesso ao drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQGA2kdK45Vc"
      },
      "outputs": [],
      "source": [
        "#Check GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk5hJolI4F89"
      },
      "source": [
        "# Topification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bzv0KGN9o_s"
      },
      "outputs": [],
      "source": [
        "#!pip install bertopic==0.12.0\n",
        "!pip install bertopic==0.16.0\n",
        "!pip install pyspark\n",
        "!pip install keyphrase-vectorizers\n",
        "!pip install sentence-transformers\n",
        "!pip install gensim==4.2.0\n",
        "!pip install joblib==1.1.1\n",
        "!pip install --upgrade pip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRTqYMMOUDOI"
      },
      "outputs": [],
      "source": [
        "#JusTop2Vec\n",
        "\n",
        "# https://github.com/ddangelov/Top2Vec\n",
        "\n",
        "from top2vec import Top2Vec\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "\n",
        "class jusTop2Vec:\n",
        "    def __init__(self, language_model='universal-sentence-encoder-multilingual', seed=2021):\n",
        "        self.LM = language_model\n",
        "        self.seed = seed\n",
        "\n",
        "        import torch\n",
        "        torch.manual_seed(self.seed)\n",
        "        torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def train(self, documents):\n",
        "        return Top2Vec(documents, embedding_model=self.LM)\n",
        "\n",
        "    def get_num_topics(self, model):\n",
        "        return model.get_num_topics()\n",
        "\n",
        "    def print_topics(self, model, num_topics=None):\n",
        "        \"\"\"\n",
        "        Print word distribution per topic\n",
        "\n",
        "        :parameters:\n",
        "            num_topics: number of topics\n",
        "        \"\"\"\n",
        "        topics_words, word_scores, topic_nums = model.get_topics(num_topics)\n",
        "\n",
        "        for topic in topic_nums:\n",
        "            print(f\"Topic %d: {topics_words[topic]}\" % topic)\n",
        "\n",
        "    def get_word_distribution_per_topic(self, model, num_topics=None):\n",
        "        \"\"\"\n",
        "        Get distribution of words per topic\n",
        "\n",
        "        :parameters:\n",
        "            num_topics: number of topics\n",
        "\n",
        "        :return:\n",
        "           distribution of words per topics\n",
        "        \"\"\"\n",
        "        topics_words, word_scores, topic_nums = model.get_topics(num_topics)\n",
        "        all_topics = {}\n",
        "        for topic in topic_nums:\n",
        "            all_topics[topic] = topics_words[topic]\n",
        "        return all_topics\n",
        "\n",
        "    def get_topics_per_doc(self, model, total_num_docs):\n",
        "        \"\"\"\n",
        "        Get distribution of topics per document\n",
        "\n",
        "        :return:\n",
        "           topic per document\n",
        "        \"\"\"\n",
        "        topic_per_document = []\n",
        "        for doc_id in range(0, total_num_docs):\n",
        "            doc_topics, doc_dist, topic_words, topic_word_scores = model.get_documents_topics([doc_id])\n",
        "            topic_per_document.append((doc_topics[0], doc_dist[0]))\n",
        "        return topic_per_document\n",
        "\n",
        "    def show_wordcloud(self, model, topic_id):\n",
        "        model.generate_topic_wordcloud(topic_id)\n",
        "\n",
        "    def search_documents_by_topic(self, model, topic_num=0, num_docs=10):\n",
        "        return model.search_documents_by_topic(topic_num, num_docs)\n",
        "\n",
        "    def get_keywords_per_topic(self, model, num_topics=1, top_n=10):\n",
        "        keywords = {}\n",
        "        words_per_topic = self.get_word_distribution_per_topic(model, num_topics=num_topics)\n",
        "        for k in range(0, num_topics):\n",
        "            wp = words_per_topic[k]\n",
        "            keywords[k] = [word for word in wp[0:top_n]]\n",
        "        return keywords\n",
        "\n",
        "    def stats_topic_per_document(self, model, corpus, doc_ids_list, num_topics=1, top_n_words=10):\n",
        "        # Init output\n",
        "        topic_csv_df = pd.DataFrame()\n",
        "        doc_csv_df = pd.DataFrame()\n",
        "\n",
        "        topics_per_doc = self.get_topics_per_doc(model, len(corpus))\n",
        "        words_per_topic = self.get_word_distribution_per_topic(model, num_topics=num_topics)\n",
        "\n",
        "        # Get main topic in each document\n",
        "        for i, row in enumerate(topics_per_doc):\n",
        "            topic_num = row[0]\n",
        "            max_score = row[1]\n",
        "            doc_csv_df = doc_csv_df.append(pd.Series([int(topic_num), round(max_score, 4)]), ignore_index=True)\n",
        "        doc_csv_df.columns = ['Dominant_Topic', 'Perc_Contribution']\n",
        "\n",
        "        # adding document id and numero unificado\n",
        "        juris_ids = pd.Series(doc_ids_list)\n",
        "        doc_csv_df = pd.concat([juris_ids, doc_csv_df], axis=1)\n",
        "\n",
        "        # Add original text to the end of the output\n",
        "        contents = pd.Series(corpus)\n",
        "        doc_csv_df = pd.concat([doc_csv_df, contents], axis=1)\n",
        "\n",
        "        # Format\n",
        "        doc_csv_df.columns = ['Document_id', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Document']\n",
        "\n",
        "        for topic_num in range(0, len(words_per_topic)):\n",
        "            wp = words_per_topic[topic_num]\n",
        "            topic_keywords = \", \".join([word for word in wp[0:top_n_words]])\n",
        "            topic_csv_df = topic_csv_df.append(pd.Series([topic_num, topic_keywords]), ignore_index=True)\n",
        "        topic_csv_df.columns = ['Topic', 'Keywords']\n",
        "\n",
        "        return doc_csv_df, topic_csv_df\n",
        "\n",
        "    def array_to_string(self, my_list, top_n_words):\n",
        "        print(my_list)\n",
        "        return \", \".join([word for word in my_list[0:top_n_words]])\n",
        "\n",
        "    def stats_topic_per_document_spark(self, spark, model, corpus, doc_ids_list, num_topics=1, top_n_words=10):\n",
        "        topics_per_doc = self.get_topics_per_doc(model, len(corpus))\n",
        "        words_per_topic = self.get_word_distribution_per_topic(model, num_topics=num_topics)\n",
        "        list_topics_per_doc = list(map(list, zip(*topics_per_doc)))\n",
        "        list_topics_per_doc[0] = [int(i) for i in list_topics_per_doc[0]]\n",
        "        list_topics_per_doc[1] = [float(i) for i in list_topics_per_doc[1]]\n",
        "        dfSchema = StructType([\n",
        "            StructField(\"Document_id\", StringType(), True),\n",
        "            StructField(\"Dominant_Topic\", IntegerType(), True),\n",
        "            StructField(\"Topic_Perc_Contrib\", DoubleType(), True),\n",
        "            StructField(\"Document\", StringType(), True)\n",
        "        ])\n",
        "        data = zip(doc_ids_list, list_topics_per_doc[0], list_topics_per_doc[1], corpus)\n",
        "        columns = ['Document_id', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Document']\n",
        "        doc_csv_df = spark.createDataFrame(data, columns, dfSchema)\n",
        "        doc_csv_df.withColumn(\"Dominant_Topic\", col(\"Dominant_Topic\").cast(\"int\"))\n",
        "\n",
        "        topics = list(map(int, words_per_topic.keys()))\n",
        "        list_of_words = [self.array_to_string(words, top_n_words) for words in words_per_topic.values()]\n",
        "        dfTopicsSchema = StructType([\n",
        "            StructField(\"Topic\", StringType(), True),\n",
        "            StructField(\"Keywords\", StringType(), True)\n",
        "        ])\n",
        "        columns = ['Topic', 'Keywords']\n",
        "        data = zip(topics, list_of_words)\n",
        "        topic_csv_df = spark.createDataFrame(data, columns, dfTopicsSchema)\n",
        "        topic_csv_df = topic_csv_df.withColumn('Keywords', topic_csv_df[\"Keywords\"])\n",
        "\n",
        "        return doc_csv_df, topic_csv_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE6wnJICgoao",
        "outputId": "1f5685c4-84f8-437e-e161-3ef0a9736fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.2.0\n",
            "The scikit-learn version is 1.2.2.\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "print(gensim.__version__)\n",
        "\n",
        "import sklearn\n",
        "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9FxFu4diPfp"
      },
      "source": [
        "## Import JusBERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwKB5rzZiYml"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.npyio import save\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "from bertopic.representation import MaximalMarginalRelevance\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "class jusBERTopic:\n",
        "    def __init__(self, seed=2021):\n",
        "        self.model = None\n",
        "        self.seed = seed\n",
        "        self.top_n_words = 20\n",
        "        nltk.download('punkt')\n",
        "\n",
        "        import torch\n",
        "        torch.manual_seed(self.seed)\n",
        "        torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def train_doc2vec(self, documents):\n",
        "        min_count = 50\n",
        "        hs = 1\n",
        "        negative = 0\n",
        "        epochs = 40\n",
        "        doc2vec_args = {\"vector_size\": 300,\n",
        "                        \"min_count\": min_count,\n",
        "                        \"window\": 15,\n",
        "                        \"sample\": 1e-5,\n",
        "                        \"negative\": negative,\n",
        "                        \"hs\": hs,\n",
        "                        \"epochs\": epochs,\n",
        "                        \"dm\": 0,\n",
        "                        \"dbow_words\": 1}\n",
        "\n",
        "        tokenized_doc = []\n",
        "        for d in documents:\n",
        "            tokenized_doc.append(word_tokenize(d, language='english'))\n",
        "        # Convert tokenized document into gensim formated tagged data\n",
        "        tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]\n",
        "        model = Doc2Vec(**doc2vec_args)\n",
        "        model.build_vocab(tagged_data)\n",
        "        model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "        #model.save('/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/doc2vec-model')\n",
        "        #model.save('/content/drive/MyDrive/COLIEE_2023/task1_test_files_2023/doc2vec-model-stopwords')\n",
        "        model.save('/content/drive/MyDrive/COLIEE_2023/task1_train_files_2023_final/doc2vec-model-stopwords')\n",
        "        return model\n",
        "\n",
        "    def doc2vec_infer(corpus, model_path):\n",
        "      # load doc2vec model\n",
        "      d2v_model = Doc2Vec.load(model_path)\n",
        "      embedding_model = d2v_model.wv\n",
        "      # tokenize documents\n",
        "      tokenized_doc = [word_tokenize(d, language='english') for d in corpus]\n",
        "      doc_infer = d2v_model.infer_vector(tokenized_doc[0], steps=40)\n",
        "      embedding = [doc_infer]\n",
        "      for document in tokenized_doc[1:]:\n",
        "          doc_infer = d2v_model.infer_vector(document, steps=40)\n",
        "          embedding = np.append(embedding, [np.array(doc_infer)], axis=0)\n",
        "      return embedding_model, embedding\n",
        "\n",
        "    def build_model(self, option, documents, embed_documents, lm_path, calculate_probabilities=False):\n",
        "        if option == 'sentence':  # sentence transformer\n",
        "            emb_model = SentenceTransformer(lm_path)\n",
        "            embedding_model = emb_model\n",
        "            embedding = emb_model.encode(embed_documents, show_progress_bar=True)\n",
        "        elif option == 'doc2vec':\n",
        "            print('Executing bertopic + doc2vec')\n",
        "            try:\n",
        "            # load pre-trained doc2vec model\n",
        "              #model = BERTopic.load('/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/BERT-model')\n",
        "              #model.save('/content/drive/MyDrive/COLIEE_2023/task1_test_files_2023/BERT-model-stopwords')\n",
        "    #Salvar o BERTopic treinado\n",
        "              model.save('/content/drive/MyDrive/COLIEE_2023/task1_train_files_2023_final/BERT-model-stopwords')\n",
        "\n",
        "\n",
        "        #Salvar os embeddings\n",
        "            except:\n",
        "              model = self.train_doc2vec(documents=documents)\n",
        "              embedding_model = model.wv\n",
        "              embedding = model.dv.vectors\n",
        "           # np.save('/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/embedding-vector-train2023', embedding)\n",
        "            np.save('/content/drive/MyDrive/COLIEE_2023/task1_train_files_2023_final/embedding-vector-train2023-stopwords', embedding)\n",
        "            #print(embedding)\n",
        "        else:  # default\n",
        "            embedding_model = None\n",
        "            embedding = None\n",
        "\n",
        "        print('Done creating embeddings.')\n",
        "\n",
        "        self.top_n_words = 20\n",
        "        # initialize model\n",
        "        representation_model = MaximalMarginalRelevance(diversity=0.3)\n",
        "        self.model = BERTopic(language='english',\n",
        "                              top_n_words=self.top_n_words,\n",
        "                              calculate_probabilities=calculate_probabilities,\n",
        "                              embedding_model=embedding_model,\n",
        "                              n_gram_range=(1, 2),\n",
        "                              verbose=True,\n",
        "                              representation_model=representation_model)\n",
        "        print('Done initializing bertopic.')\n",
        "\n",
        "        # fit model to documents\n",
        "        topics, probabilities = self.model.fit_transform(documents=documents, embeddings=embedding)\n",
        "\n",
        "        print(self.model.get_topic_freq())\n",
        "        return self.model, topics, probabilities\n",
        "\n",
        "    def get_word_distribution_per_topic(self):\n",
        "        topics = self.model.get_topics()\n",
        "        all_topics = {}\n",
        "        for topic_id in topics.keys():\n",
        "            words = []\n",
        "            for item in topics[topic_id]:\n",
        "                words.append(item[0])\n",
        "            all_topics[topic_id] = words\n",
        "        return all_topics\n",
        "\n",
        "    def get_num_topics(self):\n",
        "        return len(self.model.get_topics())\n",
        "\n",
        "    def print_topics(self):\n",
        "        list_of_words = []\n",
        "        topics = self.model.get_topics()\n",
        "        all_topics = {}\n",
        "        for topic_id in topics.keys():\n",
        "            words = []\n",
        "            for item in topics[topic_id]:\n",
        "                words.append(item[0])\n",
        "            if topic_id != -1:\n",
        "                new_words = [x for x in words if x != '']\n",
        "                list_of_words.append(new_words)\n",
        "            print(f\"Topic %d: {words}\" % topic_id)\n",
        "        return list_of_words\n",
        "\n",
        "    def stats_topic_per_document(self, all_topics, contribution, corpus, doc_ids_list):\n",
        "        # Init output\n",
        "        topic_csv_df = pd.DataFrame()\n",
        "        doc_csv_df = pd.DataFrame()\n",
        "\n",
        "        words_per_topic = self.get_word_distribution_per_topic()\n",
        "\n",
        "        topic_num = pd.Series(all_topics)\n",
        "        topic_num.astype(int)\n",
        "        max_score = pd.Series(contribution)\n",
        "        max_score = np.round(max_score, 4)\n",
        "        doc_csv_df = pd.concat([topic_num, max_score], axis=1)\n",
        "        doc_csv_df.columns = ['Dominant_Topic', 'Perc_Contribution']\n",
        "\n",
        "        # adding document id and numero unificado\n",
        "        juris_ids = pd.Series(doc_ids_list)\n",
        "        doc_csv_df = pd.concat([juris_ids, doc_csv_df], axis=1)\n",
        "\n",
        "        # Add original text to the end of the output\n",
        "        contents = pd.Series(corpus)\n",
        "        doc_csv_df = pd.concat([doc_csv_df, contents], axis=1)\n",
        "\n",
        "        # Format\n",
        "        doc_csv_df.columns = ['Document_id', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Document']\n",
        "\n",
        "        # ignoring the topic -1\n",
        "        for topic_num in range(0, len(words_per_topic)-1):\n",
        "            wp = words_per_topic[topic_num]\n",
        "            topic_keywords = \", \".join([word for word in wp])\n",
        "            topic_csv_df = topic_csv_df.append(pd.Series([topic_num, topic_keywords]), ignore_index=True)\n",
        "        topic_csv_df.columns = ['Topic', 'Keywords']\n",
        "\n",
        "        return doc_csv_df, topic_csv_df\n",
        "\n",
        "    def stats_topic_per_document_with_probabilities(self, contribution, corpus, doc_ids_list):\n",
        "        # Init output\n",
        "        topic_csv_df = pd.DataFrame()\n",
        "        doc_csv_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "        words_per_topic = self.get_word_distribution_per_topic()\n",
        "\n",
        "        # Get main topic in each document\n",
        "        for i, row in enumerate(contribution):\n",
        "            # For now I am ignoring this feature of BERTopic, that is to return topic = -1  for outliers\n",
        "            # (documents that do not have a topic assigned)\n",
        "            max_score = np.max(row)\n",
        "            topic_num = np.argmax(row)\n",
        "\n",
        "            #ind = np.argpartition(row, -20)[-20:]\n",
        "            #top5 = ind[np.argsort(row[ind])]\n",
        "            #top5 = np.flip(top5)\n",
        "            top5 = np.flip(np.argsort(row))\n",
        "            doc_csv_df = doc_csv_df.append(pd.Series([int(topic_num), round(max_score, 4), top5]), ignore_index=True)\n",
        "        doc_csv_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Top5_Topics']\n",
        "\n",
        "        # adding document id and numero unificado\n",
        "        juris_ids = pd.Series(doc_ids_list)\n",
        "        doc_csv_df = pd.concat([juris_ids, doc_csv_df], axis=1)\n",
        "\n",
        "        # Add original text to the end of the output\n",
        "        contents = pd.Series(corpus)\n",
        "        doc_csv_df = pd.concat([doc_csv_df, contents], axis=1)\n",
        "\n",
        "        # Format\n",
        "        #doc_csv_df = doc_csv_df.reset_index()\n",
        "        doc_csv_df.columns = ['Document_id', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Top5_Topics', 'Ementa']\n",
        "\n",
        "        # ignoring the topic -1\n",
        "        for topic_num in range(0, len(words_per_topic)-1):\n",
        "            wp = words_per_topic[topic_num]\n",
        "            topic_keywords = \", \".join([word for word in wp])\n",
        "            topic_csv_df = topic_csv_df.append(pd.Series([topic_num, topic_keywords]), ignore_index=True)\n",
        "        topic_csv_df.columns = ['Topic', 'Keywords']\n",
        "\n",
        "        return doc_csv_df, topic_csv_df\n",
        "\n",
        "    def array_to_string(self, my_list):\n",
        "        return \", \".join([word for word in my_list])\n",
        "\n",
        "    def stats_topic_per_document_spark(self, spark, all_topics, contribution, corpus, doc_ids_list):\n",
        "        contribution = [float(np.round(i, 4)) for i in contribution]\n",
        "        dfSchema = StructType([\n",
        "            StructField(\"Document_id\", StringType(), True),\n",
        "            StructField(\"Dominant_Topic\", IntegerType(), True),\n",
        "            StructField(\"Topic_Perc_Contrib\", DoubleType(), True),\n",
        "            StructField(\"Document\", StringType(), True)\n",
        "        ])\n",
        "        data = zip(doc_ids_list, all_topics, contribution, corpus)\n",
        "        columns = ['Document_id', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Document']\n",
        "        doc_csv_df = spark.createDataFrame(data, columns, dfSchema)\n",
        "        doc_csv_df.withColumn(\"Dominant_Topic\", col(\"Dominant_Topic\").cast(\"int\"))\n",
        "\n",
        "        words_per_topic = self.get_word_distribution_per_topic()\n",
        "        array_to_string_udf = udf(self.array_to_string, StringType())\n",
        "        dfTopicsSchema = StructType([\n",
        "            StructField(\"Topic\", StringType(), True),\n",
        "            StructField(\"Keywords\", StringType(), True)\n",
        "        ])\n",
        "        columns = ['Topic', 'Keywords']\n",
        "        data = zip(words_per_topic.keys(), words_per_topic.values())\n",
        "        topic_csv_df = spark.createDataFrame(data, columns, dfTopicsSchema)\n",
        "        topic_csv_df = topic_csv_df.withColumn('Keywords', array_to_string_udf(topic_csv_df[\"Keywords\"]))\n",
        "        return doc_csv_df, topic_csv_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-9KKV0VWYKp"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#from jusTop2Vec import jusTop2Vec\n",
        "#from jusBERTopic import jusBERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "class LoadData:\n",
        "    def load_data(self):\n",
        "        documents = []\n",
        "        list_of_ids = []\n",
        "        i=0\n",
        "\n",
        "        #mypath = '/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/'\n",
        "        mypath = '/content/drive/MyDrive/COLIEE_2023/task1_train_files_2023_final/'\n",
        "        for txt in glob(os.path.join(mypath, '*.txt')):\n",
        "          with open(txt, encoding='utf-8') as tfile:\n",
        "            text = tfile.read()\n",
        "            text = text.replace('\\n',' ').replace('FRAGMENT_SUPPRESSED','')\n",
        "            documents.append(text)\n",
        "            list_of_ids.append(txt)\n",
        "            #list_of_ids[:] = [d.replace('/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/','') for d in list_of_ids]\n",
        "            list_of_ids[:] = [d.replace('/content/drive/MyDrive/COLIEE_2023/task1_train_files_2023_final/','') for d in list_of_ids]\n",
        "\n",
        "        return documents, list_of_ids\n",
        "\n",
        "# prepare data to run topic modeling solution\n",
        "class PreprocessingData:\n",
        "    def __init__(self, min_df=1, max_df=0.75, token_pattern=r\"[a-zA-Z]\\w*\"): ##Mexer nisso?\n",
        "        self.min_df = min_df\n",
        "        self.max_df = max_df\n",
        "        self.token_pattern = token_pattern\n",
        "        # list of stop words in english & remove in french\n",
        "        nltk.download('stopwords')\n",
        "        self.en_stopwords = nltk.corpus.stopwords.words('english') # Mudado\n",
        "        self.en_stopwords = nltk.corpus.stopwords.words('french') # Mudado\n",
        "        self.en_stopwords.extend(['refd'])  # Add 'refd' as stopword\n",
        "        self.en_stopwords.extend(['de', 'la', 'rad', 'rpd', 'la citoyenneté', 'la sar', 'mahjoub', 'falun', 'pif', 'juge de', 'mr mahjoub', 'gong', 'le', 'que', 'convention', 'standard', 'la spr', 'harkat', 'falun gong', 'les', 'et', 'rpds', 'mr harkat', 'china', 'la commission', 'des', 'rads', 'demandeur', 'de la', 'almrei', 'mr shukurov', 'du', 'décision', 'mahjoubs', 'psb', 'au', 'abroad class', 'mr ajaj', 'est', 'le juge', 'la décision', 'mr almrei', 'board', 'en', 'pas', 'de létat', 'ajaj', 'un risque', 'mr mahjoubs', 'practitioner', 'lagent', 'charkaoui', 'summons', 'dans', 'une', 'country asylum', 'le demandeur', 'csis', 'gong practitioner', 'les demandeurs', 'la norme', 'bin laden', 'practice', 'il', 'que la', 'jihad', 'qui', 'asylum class', 'laden', 'practitioners', 'par', 'la juge', 'norme de', 'bin', 'décision de', 'practice falun', 'ne', 'que le', 'genuine', 'la protection', 'pour', 'preuve', 'un', 'arrest', 'quil', 'asylum', 'le risque', 'de contrôle', 'spr', 'de résidence', 'par la', 'knowledge falun', 'la cour', 'noc'])\n",
        "\n",
        "\n",
        "    def remove_punctuation(self, text):\n",
        "        \"\"\"\n",
        "        Remove punctuations\n",
        "\n",
        "        :parameters:\n",
        "            text: sentence (string)\n",
        "\n",
        "        :return: preprocessed documents without punctuations\n",
        "        \"\"\"\n",
        "        punctuation = r'[/.!$%^&#*+\\'\\\"()-.,:;<=>?@[\\]{}|]'\n",
        "        return re.sub(punctuation, '', text)\n",
        "\n",
        "    def strip_accents_from_word(self, text):\n",
        "        \"\"\"\n",
        "        Strip accents from input String.\n",
        "\n",
        "        :parameters:\n",
        "            text: word (string)\n",
        "\n",
        "        :return: preprocessed word without accent\n",
        "         \"\"\"\n",
        "        text = unicodedata.normalize('NFD', text)\n",
        "        text = text.encode('ascii', 'ignore')\n",
        "        text = text.decode(\"utf-8\")\n",
        "        return str(text)\n",
        "\n",
        "    def strip_accents(self, text):\n",
        "        \"\"\"\n",
        "        Strip accents from text.\n",
        "\n",
        "        :parameters:\n",
        "            text: sentence (string)\n",
        "\n",
        "        :return: preprocessed documents without accent\n",
        "         \"\"\"\n",
        "        return \" \".join([self.strip_accents_from_word(word) for word in text.split()])\n",
        "\n",
        "    def remove_URL(self, text):\n",
        "        \"\"\"\n",
        "        Remove URLs from text\n",
        "        :return: text without URLs\n",
        "        \"\"\"\n",
        "        return re.sub(r\"<.*?>\", \" \", text)\n",
        "\n",
        "    def clean_text(self, documents):\n",
        "        \"\"\"\n",
        "        Clean (preprocess) text data: parse URL, remove punctuation, convert to lowercase, remove stopword, and\n",
        "        strip accents\n",
        "        :param documents: list of documents\n",
        "        :return: preprocessed_text: list of preprocessed documents\n",
        "        \"\"\"\n",
        "        # preprocess jusbrasil URLs\n",
        "        preprocessed_text = []\n",
        "        for text in documents:\n",
        "            preprocessed_text.append(self.remove_URL(text))\n",
        "        # We have to decide whether we want to remove or keep punctuation\n",
        "        # agravodeinstrumentoai70068698984rs or lei_de_criacao_do_pis_lei_complementar_7_70\n",
        "        preprocessed_text = [self.remove_punctuation(doc) for doc in preprocessed_text]\n",
        "        # convert to lowercase\n",
        "        preprocessed_text = [doc.lower() for doc in preprocessed_text]\n",
        "        # remove stopwords\n",
        "        preprocessed_text = [' '.join([w for w in doc.split() if len(w) > 1 and w not in self.en_stopwords])\n",
        "                             for doc in preprocessed_text]\n",
        "        # strip accents\n",
        "        # preprocessed_text = [self.strip_accents(doc) for doc in preprocessed_text]\n",
        "        return preprocessed_text\n",
        "\n",
        "    def prepare_text_for_topicmodel(self, documents, list_doc_ids):\n",
        "        \"\"\"\n",
        "        Prepare data for BERTopic topic discovery\n",
        "        :param documents: list of documents\n",
        "        :param list_doc_ids: list of documents' ids\n",
        "        :return: preprocessed_docs: list of preprocessed documents\n",
        "                 ids_docs_removed: list of ids to be removed\n",
        "        \"\"\"\n",
        "        # clean the data: parser URL, lowercase, remove punctuation, and remove stopwords\n",
        "        preprocessed_docs_tmp = self.clean_text(documents)\n",
        "\n",
        "        # stop_word = None. Stopwords were removed in clean_text()\n",
        "        vectorizer = CountVectorizer(stop_words=None,\n",
        "                                     tokenizer=None,\n",
        "                                     min_df=self.min_df,\n",
        "                                     max_df=self.max_df,\n",
        "                                     ngram_range=(1, 2),\n",
        "                                     token_pattern=self.token_pattern,\n",
        "                                     lowercase=True)\n",
        "\n",
        "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
        "        vocabulary = set(vectorizer.get_feature_names_out())\n",
        "\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w.lower() in vocabulary])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        list_ids_removed = []\n",
        "        preprocessed_docs, unpreprocessed_docs = [], []\n",
        "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
        "            if len(doc) > 0:\n",
        "                preprocessed_docs.append(doc)\n",
        "                unpreprocessed_docs.append(documents[i])\n",
        "            else:\n",
        "                list_ids_removed.append(list_doc_ids[i])\n",
        "\n",
        "        return preprocessed_docs, list_ids_removed\n",
        "\n",
        "\n",
        "class TopicModel:\n",
        "    def __init__(self, corpus=[], embed_corpus=[], doc_ids_list=[], embedding_option='doc2vec', embedding_path=''):\n",
        "        self.corpus = corpus\n",
        "        if embed_corpus:\n",
        "            self.embed_corpus = embed_corpus\n",
        "        else:\n",
        "            self.embed_corpus = corpus\n",
        "        self.doc_ids_list = doc_ids_list\n",
        "        self.seed = 2021\n",
        "        self.embedding_option = embedding_option\n",
        "        self.embedding_path = embedding_path\n",
        "\n",
        "    def run_bertopic(self, calculate_probabilities=False):\n",
        "        \"\"\"\n",
        "        Run BERTopic and save description of topics and statistics of topics per document into csv files.\n",
        "        :param calculate_probabilities: whether to calculate the probabilities of all topics per document instead of\n",
        "        the probability of the assigned topic per document.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        bert_topic = jusBERTopic(seed=self.seed)\n",
        "        model, topics, probabilities = bert_topic.build_model(option=self.embedding_option,\n",
        "                                                              documents=self.corpus,\n",
        "                                                              embed_documents=self.embed_corpus,\n",
        "                                                              lm_path=self.embedding_path,\n",
        "                                                              calculate_probabilities=calculate_probabilities)\n",
        "        print(\"entrou no bert-topic\")\n",
        "        #model.save('/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/BERT-model-train2024')\n",
        "        model.save('/content/drive/MyDrive/COLIEE_2023/task1_train_files_2023_final/BERT-model-train2024-stopwords')\n",
        "\n",
        "        if calculate_probabilities:\n",
        "            # not optimized for spark/parquet yet\n",
        "           df_probs = pd.DataFrame(probabilities)\n",
        "           #salva a probabilidade dos tópicos\n",
        "           df_probs.to_csv('/content/drive/MyDrive/COLIEE_2023/task1_train_files_2023_final/topic-probs-train-2024-stopwords.csv')\n",
        "           #df_probs.to_csv('/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/topic-probs-train-2024.csv')\n",
        "           doc_csv_df, topic_csv_df = bert_topic.stats_topic_per_document_with_probabilities(probabilities,\n",
        "                                                                                              self.corpus,\n",
        "                                                                                              self.doc_ids_list)\n",
        "\n",
        "        else:\n",
        "            doc_csv_df, topic_csv_df = bert_topic.stats_topic_per_document(topics,\n",
        "                                                                           probabilities,\n",
        "                                                                           self.corpus,\n",
        "                                                                           self.doc_ids_list)\n",
        "\n",
        "        #doc_csv_df.to_csv('/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/bertopic_documents-train2024.csv')\n",
        "        #topic_csv_df.to_csv('/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/bertopic_topics-train2024.csv')\n",
        "        doc_csv_df.to_csv('/content/drive/MyDrive/COLIEE_2023/task1_train_files_2023_final/bertopic_documents-train2024-stopwords.csv')\n",
        "        topic_csv_df.to_csv('/content/drive/MyDrive/COLIEE_2023/task1_train_files_2023_final/bertopic_topics-train2024-stopwords.csv')\n",
        "\n",
        "\n",
        "\n",
        "class Similarity:\n",
        "    def __init__(self, embedding_path='./embedding/BERTikal'):\n",
        "        self.embedding_path = embedding_path\n",
        "\n",
        "    def compute_cosine_similarity(self, documents=[], list_of_ids=[]):\n",
        "        model = SentenceTransformer(self.embedding_path)\n",
        "        sentences_embeddings = model.encode(documents, show_progress_bar=True)\n",
        "        similarities = cosine_similarity(sentences_embeddings)\n",
        "        distances = []\n",
        "        for r in range(0, len(similarities)):\n",
        "            for c in range(r+1, len(similarities)):\n",
        "                distances.append({'Document_id_1': list_of_ids[r], 'Document_id_2': list_of_ids[c],\n",
        "                                  'Distance': similarities[c][r]})\n",
        "        df = pd.DataFrame(distances)\n",
        "        df.to_csv('distances.csv', sep=',')\n",
        "\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default='top2vec', help=\"Topic modeling algorithm (bertopic or top2vec).\")\n",
        "    return parser.parse_args()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L7Nu68KWehw"
      },
      "source": [
        "##**Main code** - Load data and run the topification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2HXpdwzWd80",
        "outputId": "c1d91965-81ff-4874-f8a6-e6f334222628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents to be removed: 0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "torch.manual_seed(2021)\n",
        "torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = \"bertopic\"\n",
        "#args = parse_args()\n",
        "\n",
        "\n",
        "### Load Data.\n",
        "ld = LoadData()\n",
        "# load data (documents and lista of ids)\n",
        "documents, list_of_ids = ld.load_data()\n",
        "\n",
        "\n",
        "# pre-process data\n",
        "if model == 'bertopic':\n",
        "    dc = PreprocessingData(min_df=1, max_df=0.75, token_pattern=r\"[a-zA-Z]\\w*\")\n",
        "    preprocessed_corpus, ids_docs_removed = dc.prepare_text_for_topicmodel(documents, list_of_ids)\n",
        "elif model == 'top2vec':\n",
        "    dc = PreprocessingData(min_df=0.01, max_df=0.75, token_pattern=r\"[a-zA-Z]\\w*\")\n",
        "    preprocessed_corpus, ids_docs_removed = dc.prepare_text_for_topicmodel(documents, list_of_ids)\n",
        "else:\n",
        "    print('Model invalid. Running default model - top2vec')\n",
        "    dc = PreprocessingData(min_df=0.01, max_df=0.5, token_pattern=r\"[a-zA-Z]\\w*\")\n",
        "    preprocessed_corpus, ids_docs_removed = dc.prepare_text_for_topicmodel(documents, list_of_ids)\n",
        "\n",
        "print('Number of documents to be removed: %d' % len(ids_docs_removed))\n",
        "if ids_docs_removed:\n",
        "    print('Documents removed: ', ids_docs_removed)\n",
        "    # stats = []\n",
        "for item in ids_docs_removed:\n",
        "    idx = list_of_ids.index(item)\n",
        "    list_of_ids.pop(idx)\n",
        "    documents.pop(idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU0TGY0CSDSB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31053d2d-a0c9-4031-a350-40be52eb0cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing bertopic + doc2vec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-07 03:00:29,127 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done creating embeddings.\n",
            "Done initializing bertopic.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-07 03:00:55,549 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-03-07 03:00:55,551 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2024-03-07 03:00:57,341 - BERTopic - Cluster - Completed ✓\n",
            "2024-03-07 03:00:57,351 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
            "2024-03-07 03:01:47,730 - BERTopic - Representation - Completed ✓\n",
            "2024-03-07 03:02:06,207 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Topic  Count\n",
            "3       -1    750\n",
            "29       0    177\n",
            "1        1    107\n",
            "18       2     91\n",
            "20       3     83\n",
            "..     ...    ...\n",
            "23     109     12\n",
            "114    110     12\n",
            "107    111     11\n",
            "106    112     11\n",
            "103    113     11\n",
            "\n",
            "[115 rows x 2 columns]\n",
            "entrou no bert-topic\n"
          ]
        }
      ],
      "source": [
        "# run topic modeling\n",
        "if model == 'bertopic':\n",
        "    tm = TopicModel(corpus=preprocessed_corpus,\n",
        "                    embed_corpus=preprocessed_corpus,\n",
        "                    doc_ids_list=list_of_ids,\n",
        "                    embedding_option='flair', #doc2vec, sentence ou flair\n",
        "                    embedding_path='') # PT model (huggingface path:  neuralmind/bert-base-portuguese-cased\n",
        "    tm.run_bertopic(calculate_probabilities=True)\n",
        "elif model == 'top2vec':\n",
        "    tm = TopicModel(corpus=preprocessed_corpus,\n",
        "                    doc_ids_list=list_of_ids,\n",
        "                    embedding_option='doc2vec',\n",
        "                    embedding_path='')\n",
        "    tm.run_top2vec()\n",
        "else:\n",
        "    print('Model invalid. Running Bertopic')\n",
        "    tm = TopicModel(corpus=preprocessed_corpus,\n",
        "                    embed_corpus=preprocessed_corpus,\n",
        "                    doc_ids_list=list_of_ids,\n",
        "                    embedding_option='doc2vec',\n",
        "                    embedding_path='') ## Alterar pra rede em ingles se for rodar esse\n",
        "    tm.run_bertopic(calculate_probabilities=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn9e9x1U4QIt"
      },
      "source": [
        "#Evaluation (ignorar daqui pra baixo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWG2Zrdvvyct"
      },
      "source": [
        "## Evaluate topification results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCa2mVPasFSK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Index(['Unnamed: 0', 'Document_id', 'Dominant_Topic', 'Topic_Perc_Contrib',\n",
        "#        'Top5_Topics', 'Ementa', 'Q', 'S'], dtye='object')\n",
        "def generate_dominant_map(csv_filepath: str) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    return dict(zip(df.Document_id, df.Dominant_Topic))\n",
        "\n",
        "\n",
        "def generate_top_five_map(csv_filepath: str) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    return dict(zip(df.Document_id, df.Top5_Topics))\n",
        "\n",
        "\n",
        "def generate_document_map(json_file: str) -> map:\n",
        "    with open(json_file) as json_data:\n",
        "        data = json.load(json_data)\n",
        "        return data\n",
        "\n",
        "def generate_top_k_map(csv_filepath: str, k: int) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    result_map = dict(zip(df.Document_id, df.Top5_Topics))\n",
        "    # print(\"Pre-filtering map:\", result_map)\n",
        "    filtered_map = remove_all_elements_greater_than_k(result_map, k)\n",
        "    #print(\"Filtered on top K map:\", filtered_map)\n",
        "    return filtered_map\n",
        "\n",
        "\n",
        "def remove_all_elements_greater_than_k(input_map : map, cut_value: int) -> map:\n",
        "    new_map = dict()\n",
        "    for (key, v) in input_map.items():\n",
        "        l = str_to_list(v)\n",
        "        new_map[key] = build_str(l, cut_value)\n",
        "    return new_map\n",
        "\n",
        "def match_dominant(doc_to_dominant_topic: map, doc_to_refs: map) -> map:\n",
        "    result = Counter()\n",
        "    for (doc, v) in doc_to_refs.items():\n",
        "        for ref in doc_to_refs[doc]:\n",
        "            if (doc_to_dominant_topic[ref] == doc_to_dominant_topic[doc]):\n",
        "                result[doc] = result[doc] + 1\n",
        "        result[doc] = result[doc] / len(doc_to_refs[doc])\n",
        "    return result\n",
        "\n",
        "#ALTERNATIVA 1: Avaliar a presença do tópico dominante da QUERY nos top k tópicos dominantes da RESPOSTA\n",
        "def dominant_inside_top_k_type1(doc_to_dominant_topic: map, doc_to_top_k: map, doc_to_refs: map) -> map:\n",
        "    result = Counter()\n",
        "    for (doc, v) in doc_to_refs.items(): #([('048771.txt', ['038112.txt', '026070.txt']), (...)])\n",
        "      try:\n",
        "        for ref in doc_to_refs[doc]: # ['038112.txt', '026070.txt']\n",
        "          query_dominant = doc_to_dominant_topic[doc] #13\n",
        "          top_k_from_ref = doc_to_top_k[ref].replace(\"[\",\"\").replace(\"]\",\"\").split(' ') # ['94', '5', '105', '3', '20']\n",
        "          if (convert_float_to_int_string(query_dominant) in top_k_from_ref):\n",
        "            result[doc] = result[doc] + 1\n",
        "      except:\n",
        "        result[doc] = result[doc] + 0\n",
        "      result[doc] = result[doc] / len(doc_to_refs[doc])\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "#ALTERNATIVA 2: #AJUSTAR O doc_to_dominant_topic pra não pegar todos os docs; Avaliar a presença do tópico dominante da RESPOSTA nos top k tópicos dominantes da QUERY ;\n",
        "def dominant_inside_top_k_type2(doc_to_dominant_topic: map, doc_to_top_k: map, doc_to_refs: map) -> map:\n",
        "    result = Counter()\n",
        "    for (doc, v) in doc_to_refs.items(): #([('048771.txt', ['038112.txt', '026070.txt']), (...)])\n",
        "      try:\n",
        "        for ref in doc_to_refs[doc]: # ['038112.txt', '026070.txt']\n",
        "          reference_dominant = doc_to_dominant_topic[ref] #94.0\n",
        "          top_five_from_doc = doc_to_top_k[doc].replace(\"[\",\"\").replace(\"]\",\"\").split(' ') # ['13', '19', '20', '35', '3', '101', '7', '107', '25', '75']\n",
        "          if (convert_float_to_int_string(reference_dominant) in top_five_from_doc):\n",
        "            result[doc] = result[doc] + 1\n",
        "      except:\n",
        "        result[doc] = result[doc] + 0\n",
        "      result[doc] = result[doc] / len(doc_to_refs[doc])\n",
        "    return result\n",
        "\n",
        "\n",
        "def convert_float_to_int_string(value: float) -> str:\n",
        "    return str(int(float(value)))\n",
        "\n",
        "def str_to_list(value: str) -> list:\n",
        "    cleaned_v = value.replace(\"]\", \"\").replace(\"[\", \"\").strip()\n",
        "    return re.split(\"\\s+\", cleaned_v)\n",
        "\n",
        "\n",
        "def build_str(l: list, cut_value: int) -> str:\n",
        "   return \"[\" + \" \".join(l[0:cut_value]) + \"]\"\n",
        "\n",
        "def references_agreement_on_topic(doc_to_dominant_topic: map, doc_to_refs: map) -> map:\n",
        "    result = Counter()\n",
        "    for (doc, v) in doc_to_refs.items(): #([('048771.txt', ['038112.txt', '026070.txt']), (...)])\n",
        "        tmp_counter = Counter()\n",
        "        for ref in doc_to_refs[doc]:\n",
        "            reference_dominant_value = doc_to_dominant_topic[ref]\n",
        "            tmp_counter[reference_dominant_value] = tmp_counter[reference_dominant_value] + 1\n",
        "        result[doc] = (tmp_counter.most_common(1)[0][0], tmp_counter.most_common(1)[0][1]/len(doc_to_refs[doc]))\n",
        "    return result\n",
        "\n",
        "def write_document_map_to_csv(map: map, filename: str) -> None:\n",
        "    print(\"To write at file: \" + \"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/\"+filename)\n",
        "    with open(\"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/\"+filename,'w',encoding = 'utf-8') as f:\n",
        "        f.write(\"document,percentual\\n\")\n",
        "        for (k,v) in map.items():\n",
        "            f.write( str(k) + \",\" + str(v) + \"\\n\" )\n",
        "\n",
        "\n",
        "def write_agreement_analysis_to_csv(map: map, filename: str) -> None:\n",
        "    print(\"To write at file: \"+\"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/\"+filename)\n",
        "    with open(\"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/\"+filename,'w+',encoding = 'utf-8') as f:\n",
        "        f.write(\"document,topic_dominant_id_agreed, percentual_of_the_agreement\\n\")\n",
        "        for (k,v) in map.items():\n",
        "            topic_agreed = convert_float_to_int_string(v[0])\n",
        "            percentual_of_agreement = str(v[1])\n",
        "            f.write( str(k) + \",\" + topic_agreed + \",\"+ percentual_of_agreement + \"\\n\" )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-6Tg_4jz0oE"
      },
      "outputs": [],
      "source": [
        "doc_to_top_k['038112.txt'].replace(\"[\",\"\").replace(\"]\",\"\").split(' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y24UkYfK3CDi"
      },
      "source": [
        "## Main code evaluation without threshold and fixed k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMJAtIFIvxi6"
      },
      "outputs": [],
      "source": [
        "# Input files\n",
        "bert_result_file = \"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/results/bertopic_documents_final-k-max.csv\"\n",
        "document_references_file = \"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/task1_train_labels.json\"\n",
        "\n",
        "# Compute the maps for analysis\n",
        "#dict: doc -> dominant topic ; 4415 items; {'021359.txt': 20.0, '028184.txt': 0.0 ....}\n",
        "doc_to_dominant_topic = generate_dominant_map(bert_result_file)\n",
        "#dict: queries -> answers ; 650 items; {'008447.txt': ['072495.txt', '082291.txt',...]...}\n",
        "doc_to_refs = generate_document_map(document_references_file)\n",
        "#dict: doc -> top K dominant topics ; 4415 items; {'021359.txt': '[20 3 19 13 105]', ...}\n",
        "doc_to_top_k_raw = generate_top_k_map(bert_result_file,10)\n",
        "\n",
        "# Generate analysis\n",
        "import statistics\n",
        "\n",
        "# 1. Recall (Dominant Match): % of the references which has the same dominant as the original document\n",
        "# dominant_matches = match_dominant(doc_to_dominant_topic, doc_to_refs)\n",
        "# print(\"Recall: \", dominant_matches)\n",
        "# mean = statistics.mean(dominant_matches.values())\n",
        "# print(mean)\n",
        "# write_document_map_to_csv(dominant_matches, \"Recall_bertopic_final.csv\")\n",
        "\n",
        "# 2. Topic Agreement on References: % of references which have the same dominant topic\n",
        "#   agreement_result = references_agreement_on_topic(doc_to_dominant_topic, doc_to_refs)\n",
        "# print(\"Topic Agreement on References: \", agreement_result, '\\n')\n",
        "# write_agreement_analysis_to_csv(agreement_result, \"agreement_result_bertopic_final.csv\")\n",
        "\n",
        "# 3. Recall top K (References inside Top 5): % of reference dominant topics which is inside the document top five\n",
        "Recall_topK = dominant_inside_top_k_type1(doc_to_dominant_topic, doc_to_top_k_raw, doc_to_refs)\n",
        "print(\"References inside Top K: \", Recall_topK, '\\n')\n",
        "\n",
        "mean3 = statistics.mean(Recall_topK.values())\n",
        "print(mean3)\n",
        "\n",
        "#   write_document_map_to_csv(refs_inside_top_k, \"top_20_bertopic_final.csv\")\n",
        "\n",
        "\n",
        "#4. Precision : number of references in the same dominant topic of the query / total number of docs in this topic\n",
        "#precision_results = precision(doc_to_dominant_topic, doc_to_refs, precision_docs)\n",
        "#print(\"Precision: \", precision_results)\n",
        "#write_document_map_to_csv(precision_results, \"precision_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w79CMB06reWi"
      },
      "outputs": [],
      "source": [
        "print(statistics.median(Recall_topK.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITj31MTG5vpU"
      },
      "source": [
        "## Adding a threshold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XrG_XNF_ksZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import statistics\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "\n",
        "#load docs\n",
        "bert_result_file = \"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/results/bertopic_documents_final-k-max.csv\"\n",
        "document_references_file = \"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/task1_train_labels.json\"\n",
        "queries_to_dominant_topic = \"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/queries.csv\"\n",
        "\n",
        "#Select >>>>candidate topics<<<< considering the Perc of Contrib from the dominant topic > threshould\n",
        "\n",
        "def generate_top_k_map_threshold(csv_filepath: str, k: int, threshold) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    result_list_raw = (pd.DataFrame(zip(df.Document_id, df.Topic_Perc_Contrib, df.Top5_Topics), columns = ['Document_id', 'Topic_Perc_Contrib', 'Top5_Topics']))\n",
        "    #print(\"Pre-filtering map:\", result_list_raw)\n",
        "    result_after_threshold = result_list_raw[result_list_raw['Topic_Perc_Contrib'] > threshold]\n",
        "    result_map = dict(zip(result_after_threshold['Document_id'],result_after_threshold['Top5_Topics']))\n",
        "    #INSERT QUERY DOCS BACK\n",
        "    query_map = generate_top_k_map(queries_to_dominant_topic,k)\n",
        "    final_map = {**query_map,**result_map}\n",
        "    #print(\"Filtered on top K map:\", result_map)\n",
        "    filtered_map = remove_all_elements_greater_than_k(final_map, k)\n",
        "    #print(\"Filtered on top K map:\", filtered_map)\n",
        "    return filtered_map\n",
        "\n",
        "\n",
        "#this one without a threshould\n",
        "def generate_top_k_map(csv_filepath: str, k: int) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    result_map = dict(zip(df.Document_id, df.Top5_Topics)) # TOP K\n",
        "    # print(\"Pre-filtering map:\", result_map)\n",
        "    filtered_map = remove_all_elements_greater_than_k(result_map, k)\n",
        "    # print(\"Filtered on top K map:\", filtered_map)\n",
        "    return filtered_map\n",
        "\n",
        "\n",
        "\n",
        "def remove_all_elements_greater_than_k(input_map : map, cut_value: int):\n",
        "    new_map = dict()\n",
        "    for (key, v) in input_map.items():\n",
        "        l = str_to_list(v)\n",
        "        new_map[key] = build_str(l, cut_value)\n",
        "    return new_map\n",
        "\n",
        "def str_to_list(value: str) -> list:\n",
        "    cleaned_v = value.replace(\"]\", \"\").replace(\"[\", \"\").strip()\n",
        "    return re.split(\"\\s+\", cleaned_v)\n",
        "\n",
        "def build_str(l: list, cut_value: int) -> str:\n",
        "   return \"[\" + \" \".join(l[0:cut_value]) + \"]\"\n",
        "\n",
        "def generate_dominant_map(csv_filepath: str) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    return dict(zip(df.Document_id, df.Dominant_Topic))\n",
        "\n",
        "def generate_document_map(json_file: str) -> map:\n",
        "    with open(json_file) as json_data:\n",
        "        data = json.load(json_data)\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jDzMsrGDsWq"
      },
      "source": [
        "## Evaluation with threshold and fixed K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oExhMf_Dzwp"
      },
      "outputs": [],
      "source": [
        "#For k = 10\n",
        "k=10\n",
        "threshold= 0.2\n",
        "\n",
        "## Compute the maps for analysis\n",
        "#dict: doc -> dominant topic ; 4415 items; {'021359.txt': 20.0, '028184.txt': 0.0 ....}\n",
        "doc_to_dominant_topic = generate_dominant_map(bert_result_file)\n",
        "\n",
        "#dict: queries -> answers ; 650 items; {'008447.txt': ['072495.txt', '082291.txt',...]...}\n",
        "doc_to_refs = generate_document_map(document_references_file)\n",
        "\n",
        "#dict: doc -> top K=5 dominant topics, threhold = 0.2 ;  items; {'021359.txt': '[20 3 19 13 105]', ...}\n",
        "doc_to_top_k_raw = generate_top_k_map_threshold(bert_result_file,k,threshold)\n",
        "\n",
        "\n",
        "# Generate analysis\n",
        "import statistics\n",
        "\n",
        "#Recall top K (References inside Top 5): % of reference dominant topics which is inside the document top five\n",
        "refs_inside_top_k = dominant_inside_top_k_type1(doc_to_dominant_topic, doc_to_top_k_raw, doc_to_refs)\n",
        "\n",
        "\n",
        "\n",
        "print(\"References inside Top K: \", refs_inside_top_k, '\\n')\n",
        "mean3 = statistics.mean(refs_inside_top_k.values())\n",
        "median = statistics.median(refs_inside_top_k.values())\n",
        "print(mean3)\n",
        "print(median)\n",
        "\n",
        "#write_document_map_to_csv(refs_inside_top_k, \"type1_top_10_with_x=04.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4txd-NYe4H8"
      },
      "source": [
        "## Main code evaluation, with threshold and K variable\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9G8MQCclakf"
      },
      "outputs": [],
      "source": [
        "#Evaluate precision considering top K dominant topics\n",
        "\n",
        "#Vary K from 5 to 115, adding 5\n",
        "k = list(range(5,116,5))\n",
        "\n",
        "#load docs\n",
        "bert_result_file = \"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/bertopic_documents_final-k-max.csv\"\n",
        "document_references_file = \"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/task1_train_labels.json\"\n",
        "\n",
        "# Compute the maps for analysis\n",
        "doc_to_dominant_topic = generate_dominant_map(bert_result_file) #dict: doc -> dominant topic ; 4415 items\n",
        "doc_to_refs = generate_document_map(document_references_file) #dict: query -> answers ; 650 items (queries)\n",
        "\n",
        "\n",
        "#generate many maps\n",
        "top_k = []\n",
        "for i in k:\n",
        "  #top_k.append(generate_top_k_map(bert_result_file,i)) # no threshould considered\n",
        "  top_k.append(generate_top_k_map_threshold(bert_result_file,i,threshold))\n",
        "\n",
        "answers = []\n",
        "for key in top_k:\n",
        " try:\n",
        "  recall_k = dominant_inside_top_k_type1(doc_to_dominant_topic, key, doc_to_refs)\n",
        "  avg_recall = statistics.mean(recall_k.values())\n",
        "  answers.append(avg_recall)\n",
        " except:\n",
        "  continue\n",
        "\n",
        "#print(top_k)\n",
        "\n",
        "\n",
        "\n",
        "# to convert lists to dictionary\n",
        "#d = dict(zip(k, answers))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLT4FFY_Xj_G"
      },
      "outputs": [],
      "source": [
        "for number in answers:\n",
        "  print(\"{:10.4f}\".format(number))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_vZ8mLhw9zS"
      },
      "outputs": [],
      "source": [
        "#plot recall vs K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#k.insert(0, '1')\n",
        "x = k\n",
        "#answers.insert(0,'0.46')\n",
        "y=answers\n",
        "\n",
        "plt.xlabel(\"K*5\")\n",
        "plt.ylabel(\"Recall\")\n",
        "plt.title('threshold = 0.2')\n",
        "plt.plot(y, marker = 'o')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWg9znKOQ9r0"
      },
      "source": [
        "## Retrieval function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bU-fTeL1g3Bj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def read_list_of_ids(file_path: str):\n",
        "    with open(file_path) as infile:\n",
        "        id_lists = infile.read().splitlines()\n",
        "    return id_lists\n",
        "\n",
        "\n",
        "# Input files\n",
        "bert_result_file = \"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/results/bertopic_documents-final.csv\"\n",
        "document_references_file = \"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/task1_train_labels.json\"\n",
        "embeddings_bin = np.load(\"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/embedding-vector.npy\")\n",
        "embeddings_list = embeddings_bin.tolist()\n",
        "list_of_ids = read_list_of_ids(\"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/list_of_ids\")\n",
        "\n",
        "\n",
        "# print(embeddings_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI1asFBCdSVe"
      },
      "outputs": [],
      "source": [
        "def generate_dominant_map(csv_filepath: str) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    return dict(zip(df.Document_id, df.Dominant_Topic))\n",
        "\n",
        "def generate_ids_to_embeddings(csv_filepath: str) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\";\")\n",
        "    return dict(zip(df.Topic, df.Embedding))\n",
        "\n",
        "def generate_top_k_map(csv_filepath: str, k: int) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    result_map = dict(zip(df.Document_id, df.Top5_Topics)) # TOP 5\n",
        "    # print(\"Pre-filtering map:\", result_map)\n",
        "    filtered_map = remove_all_elements_greater_than_k(result_map, k)\n",
        "    # print(\"Filtered on top K map:\", filtered_map)\n",
        "    return filtered_map\n",
        "\n",
        "\n",
        "def remove_all_elements_greater_than_k(input_map : map, cut_value: int):\n",
        "    new_map = dict()\n",
        "    for (key, v) in input_map.items():\n",
        "        l = str_to_list(v)\n",
        "        new_map[key] = build_str(l, cut_value)\n",
        "    return new_map\n",
        "\n",
        "\n",
        "def str_to_list(value: str) -> list:\n",
        "    cleaned_v = value.replace(\"]\", \"\").replace(\"[\", \"\").strip()\n",
        "    return re.split(\"\\s+\", cleaned_v)\n",
        "\n",
        "\n",
        "def generate_document_map(json_file: str) -> map:\n",
        "    with open(json_file) as json_data:\n",
        "        data = json.load(json_data)\n",
        "        return data\n",
        "\n",
        "def build_str(l: list, cut_value: int) -> str:\n",
        "   return \"[\" + \" \".join(l[0:cut_value]) + \"]\"\n",
        "\n",
        "\n",
        "def convert_float_to_int_string(value: float) -> str:\n",
        "    return str(int(float(value)))\n",
        "\n",
        "\n",
        "##Retrieval method: docs which have the dominant topic of the\n",
        "# Compute the maps for analysis:\n",
        "  #Doc -> Dominant topic\n",
        "all_docs_to_dominant_topic = generate_dominant_map(bert_result_file)\n",
        "#print(all_docs_to_dominant_topic) # OK\n",
        "queries_to_refs = generate_document_map(document_references_file)\n",
        "#print(queries_to_refs) # OK\n",
        "\n",
        "all_docs_to_top_k = generate_top_k_map(bert_result_file, 10) #top K = 10\n",
        "#print(all_docs_to_top_k) # OK\n",
        "\n",
        "\n",
        "# Group docs with the same dominant topic [and define top K] <<<<<\n",
        "\n",
        "# Generate list of docs with same dominant topic\n",
        "def add_to_inverted_list(inverted_map, doc, topic):\n",
        "  if topic not in inverted_map:\n",
        "    inverted_map[topic] = set()\n",
        "  inverted_map[topic].add(doc)\n",
        "\n",
        "\n",
        "def get_inverted_map(all_docs_to_dominant_topic):\n",
        "  inverted_map = dict()\n",
        "  for (doc_i, topic_i) in all_docs_to_dominant_topic.items():\n",
        "    add_to_inverted_list(inverted_map, doc_i, topic_i)\n",
        "  return inverted_map\n",
        "\n",
        "inverted_map = get_inverted_map(all_docs_to_dominant_topic)\n",
        "\n",
        "def queries_to_docs(queries, inverted_map, all_docs_to_top_k):\n",
        "    query_candidates = dict()\n",
        "    for query in queries:\n",
        "        top_k_list = str_to_list(all_docs_to_top_k[query]) # '017190.txt': '[66 105 19 35]'\n",
        "        query_candidates[query] = set()\n",
        "        for topic in top_k_list:\n",
        "            inv_list = inverted_map[float(topic)]\n",
        "  #          print(\"For dominant:\", topic, \"size: \", len(inv_list))\n",
        "            query_candidates[query] = query_candidates[query].union(inv_list)\n",
        "   #         print(\"Candidates after union with topic:\", topic, \", size: \",len(query_candidates[query]))\n",
        "    #        print(\"Final candidates for topic \", topic, \", size:\", len(query_candidates[query]))\n",
        "    return query_candidates\n",
        "\n",
        "def write_doc_to_embeddings(map: map, filename: str) -> None:\n",
        "  #  print(\"To write at file: \" + \"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/\"+filename)\n",
        "    with open(\"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/\"+filename,'w',encoding = 'utf-8') as f:\n",
        "        f.write(\"query,candidates_embeddings\\n\")\n",
        "        for (k,v) in map.items():\n",
        "            f.write( str(k) + \";\" + str(v) + \"\\n\" )\n",
        "\n",
        "\n",
        "query_candidates = queries_to_docs(queries_to_refs.keys(), inverted_map, all_docs_to_top_k)\n",
        "# print(queryToDocs['008447.txt'])\n",
        "# print(inverted_map)\n",
        "# print(inverted_map[21.0].union(inverted_map[18.0]))\n",
        "# print(len(inverted_map[117.0]))\n",
        "\n",
        "def convert_doc_ids_to_embeddings(query_candidates :map, ids_to_embeddings: map) -> map:\n",
        "    query_embeddings = dict()\n",
        "    for (query, candidates) in query_candidates.items():\n",
        "        query_embeddings[query] = [ids_to_embeddings[candidate] for candidate in candidates]\n",
        "    return query_embeddings\n",
        "\n",
        "\n",
        "\n",
        "ids_to_embeddings = dict(zip(list_of_ids, embeddings_list))\n",
        "# print(ids_to_embeddings)\n",
        "\n",
        "query_to_embeddings = convert_doc_ids_to_embeddings(query_candidates, ids_to_embeddings)\n",
        "# print(query_to_embeddings)\n",
        "#write_doc_to_embeddings(query_to_embeddings, \"query_to_embeddings\")\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# key=query, values=candidates\n",
        "\n",
        "results_raw=dict()\n",
        "results = dict()\n",
        "for key, values in query_to_embeddings.items():\n",
        "  X = np.array(ids_to_embeddings[key]).reshape(1,-1)\n",
        "  Y = values\n",
        "  similarities = cosine_similarity(X, Y)\n",
        "  sim = similarities[0]\n",
        "  lenght = len(queries_to_refs[key])\n",
        "  ind = np.argpartition(sim, lenght)[lenght:]\n",
        "  #ind = np.argpartition(sim, -35)[-35:]   # Pra variar o top N mais similares; pega os N últimos, pq está em ordem crescente;\n",
        "  top = ind[np.argsort(sim[ind])] # Retorna o indice (original) dos valores no Top N\n",
        "  value = list(query_candidates[key])\n",
        "  answers_raw= []\n",
        "  answers = dict()\n",
        "  for idx in top:\n",
        "    if value[idx] != key:\n",
        "      answers_raw.append(value[idx])\n",
        "      answers[value[idx]] = sim[idx]\n",
        "  results[key] = answers\n",
        "  results_raw[key] = answers_raw\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wLIoyb7cp2R"
      },
      "source": [
        "### Final results evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw7GZLRYWmwF"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "#Checking the biggest number of answers\n",
        "print(queries_to_refs,'\\n')\n",
        "length_dict = {key: len(value) for key, value in queries_to_refs.items()}\n",
        "print(length_dict)\n",
        "print(max(length_dict.values()), 'is the maximum number of answers to a query')\n",
        "\n",
        "def write_document_map_to_csv(map: map, filename: str) -> None:\n",
        "    print(\"To write at file: \" + \"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/evaluation/\"+filename)\n",
        "    with open(\"/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/evaluation/\"+filename,'w',encoding = 'utf-8') as f:\n",
        "        f.write(\"document,percentual\\n\")\n",
        "        for (k,v) in map.items():\n",
        "            f.write( str(k) + \",\" + str(v) + \"\\n\" )\n",
        "\n",
        "### Def Precision at N (N = len(results_raw) = 11)\n",
        "def precision_N(results_raw, queries_to_refs) -> map:\n",
        "    precision_results= Counter()\n",
        "    for query,candidates in results_raw.items():\n",
        "      for candidate in results_raw[query][::-1]:\n",
        "        if candidate in queries_to_refs[query]:\n",
        "          precision_results[query] = precision_results[query] +1\n",
        "      precision_results[query] = precision_results[query] / len(results_raw[query])\n",
        "    return precision_results\n",
        "\n",
        "### Def Recall at N (N = len(results_raw))\n",
        "def recall_N(results_raw, queries_to_refs) ->map:\n",
        "  recall_results= Counter()\n",
        "  for query,candidates in results_raw.items():\n",
        "    for candidate in results_raw[query][::-1]:\n",
        "      if candidate in queries_to_refs[query]:\n",
        "        recall_results[query] = recall_results[query] +1\n",
        "    recall_results[query] = recall_results[query] / len(queries_to_refs[query])\n",
        "  return recall_results\n",
        "\n",
        "\n",
        "\n",
        "##Calculate results\n",
        "#Calculate Precision\n",
        "precision_N = precision_N(results_raw, queries_to_refs)\n",
        "print(precision_N)\n",
        "precision_N_mean = statistics.mean(precision_N.values())\n",
        "print(precision_N_mean)\n",
        "#write_document_map_to_csv(precision_N,'Final_precision_N.csv')\n",
        "\n",
        "#Calculate Recall\n",
        "recall_N = recall_N(results_raw, queries_to_refs)\n",
        "#print(recall_N)\n",
        "recall_N_mean = statistics.mean(recall_N.values())\n",
        "print(recall_N_mean)\n",
        "#write_document_map_to_csv(recall_N,'Final_recall_N.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxDiv588Fv2c"
      },
      "outputs": [],
      "source": [
        "mean_dict = statistics.mean(length_dict.values())\n",
        "print(mean_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7siMi23WfmI"
      },
      "outputs": [],
      "source": [
        "print(recall_N)\n",
        "print(recall_N['008447.txt'])\n",
        "print('candidadates: ',results['008447.txt'])\n",
        "print('gabarito: ', queries_to_refs['008447.txt'][::-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2Fa5sL4krxq"
      },
      "source": [
        "# DEBUGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN3a8aDlHBCh"
      },
      "outputs": [],
      "source": [
        "print(doc_to_dominant_topic['008653.txt'])\n",
        "\n",
        "print(doc_to_top_k_raw[\"093015.txt\"])\n",
        "print(result_list_raw.loc[result_list_raw[\"Document_id\"] == \"093015.txt\"])\n",
        "\n",
        "print(doc_to_top_k_raw[\"031152.txt\"])\n",
        "print(result_list_raw.loc[result_list_raw[\"Document_id\"] == \"031152.txt\"])\n",
        "\n",
        "print(doc_to_top_k_raw[\"035854.txt\"])\n",
        "print(result_list_raw.loc[result_list_raw[\"Document_id\"] == \"035854.txt\"])\n",
        "\n",
        "\n",
        "print(doc_to_top_k_raw[\"061597.txt\"])\n",
        "print(result_list_raw.loc[result_list_raw[\"Document_id\"] == \"061597.txt\"])\n",
        "\n",
        "print(doc_to_top_k_raw[\"088175.txt\"])\n",
        "print(result_list_raw.loc[result_list_raw[\"Document_id\"] == \"088175.txt\"])\n",
        "\n",
        "\n",
        "\n",
        "#print(result_list_raw.loc[result_list_raw[\"Document_id\"] == \"051897.txt\"])\n",
        "\n",
        "\n",
        "\n",
        "#print(doc_to_top_k['050912.txt'])\n",
        "#print(doc_to_top_k['051897.txt'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcEdCEq1_oDX"
      },
      "outputs": [],
      "source": [
        "#TEST: result final  '066194.txt': 6\n",
        "# '066194.txt': [\"081206.txt\", \"026970.txt\", \"077831.txt\", \"051354.txt\", \"050927.txt\", \"030530.txt\", \"002428.txt\", \"086928.txt\", \"007418.txt\", \"092405.txt\", \"034038.txt\", \"094249.txt\", \"051831.txt\"]\n",
        "\n",
        "#doc_to_top_k_raw[\"038025.txt\"]\n",
        "\n",
        "result = Counter()\n",
        "try:\n",
        "  for ref in doc_to_refs[\"066194.txt\"]:\n",
        "    print(ref)\n",
        "    query_dominant = doc_to_dominant_topic[\"066194.txt\"] #94.0\n",
        "    print(query_dominant)\n",
        "    top_k_from_ref = doc_to_top_k_raw[ref].replace(\"[\",\"\").replace(\"]\",\"\").split(' ')\n",
        "    print(top_k_from_ref)\n",
        "    if (convert_float_to_int_string(query_dominant) in top_k_from_ref):\n",
        "      result[\"066194.txt\"] = result[\"066194.txt\"] + 1\n",
        "      print(result)\n",
        "except:\n",
        "    result[\"066194.txt\"] = result[\"066194.txt\"] + 0\n",
        "    print(result)\n",
        "\n",
        "result[\"066194.txt\"] = result[\"066194.txt\"] / len(doc_to_refs[\"066194.txt\"])\n",
        "\n",
        "\n",
        "\n",
        "print(len(doc_to_refs[\"066194.txt\"]))\n",
        "print(result)\n",
        "\n",
        "print(6/13)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjS_BieZRscs"
      },
      "outputs": [],
      "source": [
        "#from nltk.metrics.scores import recall\n",
        "\n",
        "  #Verificação PRECISION\n",
        "#print(results_raw.items())\n",
        "#query = '008447.txt'\n",
        "#print('candidadates: ',results_raw[query])\n",
        "#print('gabarito: ', queries_to_refs[query][::-1])\n",
        "#print(len(results_raw[query]))\n",
        "#print(precision_N[query])\n",
        "#print('precisao: ', 4/34)\n",
        "\n",
        "##Recall\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "#4. Pre-selection based on the topics:\n",
        "#Select only K-Candidate set = docs in the same top K topic as Q\n",
        "-Para doc ID Q\n",
        "  -Olha para as referências de Q\n",
        "    -Se o reference dominant topics is within the document top K topics\n",
        "      -Salva no vetor pre-selection deste doc\n",
        "\n",
        "#5. Select the X most similar docs with Q and K-Candidate set\n",
        "1. Selecionar do docs_embedding apenas os documentos da lista de pre-selection\n",
        "2. Roda o algoritmo de similaridade\n",
        "3. Recupera os X mais similares\n",
        "4. Cria map : doc_id_Q -> doc_id_Answers\n",
        "\n",
        "->Fazer isso para todos os itens do vetor\n",
        "\n",
        "\n",
        "# sm = Similarity()\n",
        "# sm.compute_cosine_similarity(preprocessed_corpus, list_of_ids)\n",
        "\n",
        "#Select top X\n",
        "\n",
        "#embed_query = [Doc2Vec(query)]\n",
        "#embed_candidatos = [[embed_doc1], [embed_doc2], [embed_doc3]]\n",
        "\n",
        "#Calculate the cosine similarity\n",
        "#from sklearn.metrics.pairwise import cosine_similarity\n",
        "#sim = cosine_similarity(X=embed_query, Y=embed_candidatos)\n",
        "\n",
        "#ind = np.argpartition(sim, -20)[-20:]\n",
        "#top = ind[np.argsort(sim[ind])]\n",
        "\n",
        "'''\n",
        "\n",
        "#print(doc_to_dominant_topic)\n",
        "#print(inverted_map)\n",
        "#print(inverted_map[21.0].union(inverted_map[18.0]))\n",
        "# print(len(inverted_map[117.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Om92zLoYhWd"
      },
      "outputs": [],
      "source": [
        "#save list_of_ids\n",
        "# open file in write mode\n",
        "with open(r'/content/drive/MyDrive/COLIEE-2024/task1_train_files_2024/list_of_ids', 'w') as fp:\n",
        "    for id in list_of_ids:\n",
        "        # write each item on a new line\n",
        "        fp.write(\"%s\\n\" % id)\n",
        "    print('Done')\n",
        "\n",
        "print(list_of_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxN1n4sGx6x_"
      },
      "outputs": [],
      "source": [
        "#Precision\n",
        "\n",
        "def precision(doc_to_dominant_topic: map, doc_to_refs: map, lista_precision):\n",
        " result = Counter()\n",
        " for (doc, v) in doc_to_refs.items():\n",
        "   for ref in doc_to_refs[doc]:\n",
        "    if (doc_to_dominant_topic[ref] == doc_to_dominant_topic[doc]):\n",
        "     result[doc] = result[doc] + 1\n",
        "   result[doc] = result[doc] / precision_docs[doc_to_dominant_topic[doc]]\n",
        " return result\n",
        "\n",
        "print(precision_docs, '\\n')\n",
        "print(doc_to_dominant_topic)\n",
        "print(precision_docs[doc_to_dominant_topic['021359.txt']])\n",
        "\n",
        "#print(precision_docs, '\\n')\n",
        "#print(doc_to_dominant_topic, '\\n')\n",
        "#print(doc_to_refs['030394.txt'],'\\n')\n",
        "#print(doc_to_refs.items(), '\\n')\n",
        "#print(precision_docs[doc_to_dominant_topic['030394.txt']])\n",
        "\n",
        "def precision(doc_to_dominant_topic: map, doc_to_refs: map, lista_precision):\n",
        " result = Counter()\n",
        " for (doc, v) in doc_to_refs.items():\n",
        "   for ref in doc_to_refs[doc]:\n",
        "    if (doc_to_dominant_topic[ref] == doc_to_dominant_topic[doc]):\n",
        "     result[doc] = result[doc] + 1\n",
        "   result[doc] = result[doc] / precision_docs[doc_to_dominant_topic[doc]]\n",
        " return result\n",
        "\n",
        "# transform doc_to_dominant_topic in df\n",
        "doc_to_dominant_df = pd.DataFrame(doc_to_dominant_topic.items(), columns=['Doc', 'Topic'])\n",
        "# Count how many times each topic is found (most common)\n",
        "series = doc_to_dominant_df['Topic'].value_counts()\n",
        "# Convert to data frame\n",
        "precision_docs = series.to_frame('Total_Docs')\n",
        "# Add index as a new column\n",
        "precision_docs['Topic'] = list(precision_docs.index)\n",
        "# Reset the index\n",
        "precision_docs = precision_docs.sort_values('Topic')\n",
        "precision_docs = precision_docs.reset_index(drop=True)\n",
        "precision_docs = precision_docs.reindex(columns=['Topic','Total_Docs'])\n",
        "#convert to dict\n",
        "precision_docs = dict(precision_docs.values)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UWG2Zrdvvyct",
        "y24UkYfK3CDi",
        "ITj31MTG5vpU"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}